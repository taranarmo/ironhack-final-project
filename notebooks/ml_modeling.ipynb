{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Censorship Detection with Machine Learning\n",
    "\n",
    "This notebook applies machine learning models to the preprocessed ASN connectivity data to detect censorship events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "Let's load the data we preprocessed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "df = pd.read_csv('../data/processed/preprocessed_for_ml.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Countries: {df['country'].nunique()}\")\n",
    "\n",
    "# Convert date column back to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"\\nSample of data:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Check and Target Analysis\n",
    "\n",
    "Let's examine the target variables and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential target variables\n",
    "target_cols = [col for col in df.columns if 'censorship_indicator' in col]\n",
    "print(f\"Potential target columns: {target_cols}\")\n",
    "\n",
    "# Analyze target distributions\n",
    "for target_col in target_cols:\n",
    "    if target_col in df.columns:\n",
    "        target_counts = df[target_col].value_counts()\n",
    "        print(f\"\\n{target_col} distribution:\")\n",
    "        print(target_counts)\n",
    "        print(f\"Positive ratio: {df[target_col].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no censorship indicators were found, create based on significant drops in connectivity\n",
    "if not any(col for col in df.columns if 'censorship_indicator' in col):\n",
    "    print(\"Creating target based on significant drops in foreign connectivity...\")\n",
    "    \n",
    "    # If we have foreign_neighbours_share, create target based on significant drops\n",
    "    if 'foreign_neighbours_share' in df.columns:\n",
    "        # Calculate country-specific rolling baseline\n",
    "        df['foreign_conn_baseline'] = df.groupby('country')['foreign_neighbours_share'].transform(\n",
    "            lambda x: x.rolling(window=30, min_periods=7).median()\n",
    "        )\n",
    "        \n",
    "        # Create binary target for significant drops\n",
    "        df['censorship_target'] = (\n",
    "            (df['foreign_neighbours_share'] < df['foreign_conn_baseline'] * 0.5) &\n",
    "            (df['foreign_neighbours_share'].notna()) &\n",
    "            (df['foreign_conn_baseline'].notna())\n",
    "        ).astype(int)\n",
    "        \n",
    "        target_col = 'censorship_target'\n",
    "    else:\n",
    "        # If we have ASN count, create target based on significant drops\n",
    "        if 'asn_count' in df.columns:\n",
    "            df['asn_count_baseline'] = df.groupby('country')['asn_count'].transform(\n",
    "                lambda x: x.rolling(window=30, min_periods=7).median()\n",
    "            )\n",
    "            \n",
    "            df['censorship_target'] = (\n",
    "                (df['asn_count'] < df['asn_count_baseline'] * 0.5) &\n",
    "                (df['asn_count'].notna()) &\n",
    "                (df['asn_count_baseline'].notna())\n",
    "            ).astype(int)\n",
    "            \n",
    "            target_col = 'censorship_target'\n",
    "        else:\n",
    "            # Fallback: use any numeric column with significant changes\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            # Pick a column that has reasonable variance\n",
    "            for col in numeric_cols:\n",
    "                if df[col].std() > 0 and df[col].nunique() > 10:\n",
    "                    df['censorship_target'] = df[col]\n",
    "                    # Normalize and create binary classification problem\n",
    "                    median_val = df['censorship_target'].median()\n",
    "                    df['censorship_target'] = (df['censorship_target'] < median_val).astype(int)\n",
    "                    target_col = 'censorship_target'\n",
    "                    break\n",
    "        \n",
    "    print(f\"Created target variable {target_col} with {df[target_col].sum()} positive cases ({df[target_col].mean():.4f} ratio)\")\n",
    "else:\n",
    "    # Use existing target\n",
    "    for col in df.columns:\n",
    "        if 'censorship_indicator' in col and df[col].sum() > 0:\n",
    "            target_col = col\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Select appropriate features for the model, excluding date, country, and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to exclude\n",
    "exclude_cols = ['date', 'country', target_col]  # Exclude target column\n",
    "\n",
    "# Add any other columns that shouldn't be features\n",
    "exclude_cols.extend([col for col in df.columns if 'baseline' in col or 'rolling_median' in col])\n",
    "\n",
    "# Select only numeric features\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in exclude_cols \n",
    "                and df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"Number of features selected: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols[:15]}... ({'truncated' if len(feature_cols) > 15 else 'complete'})\")\n",
    "\n",
    "# Prepare feature matrix X and target vector y\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Positive target ratio: {y.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance\n",
    "\n",
    "Censorship events are typically rare, so we'll need to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "for cls in classes:\n",
    "    count = (y == cls).sum()\n",
    "    print(f\"  Class {cls}: {count} samples ({count/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Split the data for training and testing. We'll use a time-based split to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date back to X for time-based splitting\n",
    "X_with_date = X.copy()\n",
    "X_with_date['date'] = df['date']\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "X_sorted = X_with_date.sort_values('date')\n",
    "y_sorted = y.loc[X_sorted.index]\n",
    "\n",
    "# Remove date column from features before modeling\n",
    "X_final = X_sorted.drop('date', axis=1)\n",
    "\n",
    "# Use the last 20% for testing (time-based split)\n",
    "split_idx = int(len(X_final) * 0.8)\n",
    "\n",
    "X_train = X_final.iloc[:split_idx]\n",
    "X_test = X_final.iloc[split_idx:]\n",
    "y_train = y_sorted.iloc[:split_idx]\n",
    "y_test = y_sorted.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Positive cases in training: {y_train.sum()} ({y_train.mean():.4f} ratio)\")\n",
    "print(f\"Positive cases in test: {y_test.sum()} ({y_test.mean():.4f} ratio)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train multiple models to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42, \n",
    "        class_weight=class_weight_dict,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        class_weight=class_weight_dict,\n",
    "        n_jobs=-1,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize scaler for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled features for Logistic Regression, original for Random Forest\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"  Confusion Matrix:\")\n",
    "    print(f\"    {cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Results\n",
    "\n",
    "Analyze the results and generate detailed reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "for name in models.keys():\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_test, results[name]['y_pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "for name in models.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[name]['y_pred_proba'])\n",
    "    auc = results[name]['auc']\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Censorship Detection Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_model = models['Random Forest']\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_final.columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(feature_importance_df.head(15))\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "sns.barplot(data=top_features, y='feature', x='importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "Summarize the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'Precision': [results[name]['precision'] for name in results.keys()],\n",
    "    'Recall': [results[name]['recall'] for name in results.keys()],\n",
    "    'F1-Score': [results[name]['f1'] for name in results.keys()],\n",
    "    'AUC': [results[name]['auc'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = summary_df.loc[summary_df['F1-Score'].idxmax(), 'Model']\n",
    "print(f\"\\nBest Model: {best_model_name} (F1-Score: {summary_df['F1-Score'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Summarize results and suggest improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CENSORSHIP DETECTION MODELING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features used: {len(feature_cols)}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Positive cases: {y.sum()} ({y.mean():.4f} ratio)\")\n",
    "print(f\"Time range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Countries: {df['country'].nunique()}\")\n",
    "print()\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  - F1-Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"  - AUC: {metrics['auc']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
 "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}