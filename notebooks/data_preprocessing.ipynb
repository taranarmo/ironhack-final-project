{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASN Censorship Detection - Data Preprocessing Notebook\n",
    "\n",
    "This notebook processes ASN connectivity data to detect potential censorship events. We'll load the exported CSVs, perform data cleaning, feature engineering, and exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Exported CSV Data\n",
    "\n",
    "First, let's load the exported CSV files from the database dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported CSVs\n",
    "asn_data = pd.read_csv('../data/raw/asn_data.csv')\n",
    "connectivity_data = pd.read_csv('../data/raw/connectivity_data.csv')\n",
    "neighbour_data = pd.read_csv('../data/raw/neighbour_data.csv')\n",
    "country_stat_data = pd.read_csv('../data/raw/country_stat_data.csv')\n",
    "\n",
    "print(\"ASN Data Shape:\", asn_data.shape)\n",
    "print(\"Connectivity Data Shape:\", connectivity_data.shape)\n",
    "print(\"Neighbour Data Shape:\", neighbour_data.shape)\n",
    "print(\"Country Stat Data Shape:\", country_stat_data.shape)\n",
    "\n",
    "print(\"\\nASN Data Columns:\", asn_data.columns.tolist())\n",
    "print(\"\\nConnectivity Data Columns:\", connectivity_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview and Quality Check\n",
    "\n",
    "Let's examine the data structure and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of ASN data\n",
    "print(\"ASN Data Info:\")\n",
    "print(asn_data.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(asn_data.head())\n",
    "\n",
    "print(\"\\nMissing values in ASN data:\")\n",
    "print(asn_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of connectivity data\n",
    "print(\"Connectivity Data Info:\")\n",
    "print(connectivity_data.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(connectivity_data.head())\n",
    "\n",
    "print(\"\\nMissing values in connectivity data:\")\n",
    "print(connectivity_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's process and combine the different data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ASN data\n",
    "print(\"Processing ASN data...\")\n",
    "asn_df = asn_data.copy()\n",
    "\n",
    "# Convert date column\n",
    "asn_df['a_date'] = pd.to_datetime(asn_df['a_date'])\n",
    "\n",
    "# Sort by country and date\n",
    "asn_df = asn_df.sort_values(['a_country_iso2', 'a_date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"ASN data processed: {len(asn_df)} records\")\n",
    "print(f\"Countries covered: {asn_df['a_country_iso2'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process connectivity data\n",
    "print(\"Processing connectivity data...\")\n",
    "conn_df = connectivity_data.copy()\n",
    "\n",
    "# Convert date column\n",
    "conn_df['date'] = pd.to_datetime(conn_df['date'])\n",
    "\n",
    "# Sort by country and date\n",
    "conn_df = conn_df.sort_values(['asn_country', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Connectivity data processed: {len(conn_df)} records\")\n",
    "print(f\"Countries covered: {conn_df['asn_country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process country stat data\n",
    "print(\"Processing country stat data...\")\n",
    "stat_df = country_stat_data.copy()\n",
    "\n",
    "# Convert timestamp column\n",
    "stat_df['cs_stats_timestamp'] = pd.to_datetime(stat_df['cs_stats_timestamp'])\n",
    "\n",
    "# Sort by country and date\n",
    "stat_df = stat_df.sort_values(['cs_country_iso2', 'cs_stats_timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Country stat data processed: {len(stat_df)} records\")\n",
    "print(f\"Countries covered: {stat_df['cs_country_iso2'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now let's create meaningful features for censorship detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based features\n",
    "def add_time_features(df, date_col):\n",
    "    df = df.copy()\n",
    "    df['year'] = df[date_col].dt.year\n",
    "    df['month'] = df[date_col].dt.month\n",
    "    df['day'] = df[date_col].dt.day\n",
    "    df['day_of_week'] = df[date_col].dt.dayofweek\n",
    "    df['day_of_year'] = df[date_col].dt.dayofyear\n",
    "    df['week_of_year'] = df[date_col].dt.isocalendar().week\n",
    "    df['quarter'] = df[date_col].dt.quarter\n",
    "    df['is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    return df\n",
    "\n",
    "# Apply to ASN data\n",
    "asn_df = add_time_features(asn_df, 'a_date')\n",
    "print(\"Added time features to ASN data\")\n",
    "\n",
    "# Apply to connectivity data\n",
    "conn_df = add_time_features(conn_df, 'date')\n",
    "print(\"Added time features to connectivity data\")\n",
    "\n",
    "# Apply to country stat data\n",
    "stat_df = add_time_features(stat_df, 'cs_stats_timestamp')\n",
    "print(\"Added time features to country stat data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features for ASN data\n",
    "def create_lagged_features(df, value_col, lags=[1, 7, 14, 30], group_col='a_country_iso2'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{value_col}_lag_{lag}'] = df.groupby(group_col)[value_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to key metrics in ASN data\n",
    "asn_df = create_lagged_features(asn_df, 'asn_count')\n",
    "print(\"Added lagged features to ASN data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling statistics features\n",
    "def create_rolling_features(df, value_col, window=7, group_col='a_country_iso2'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[f'{value_col}_rolling_mean_{window}'] = df.groupby(group_col)[value_col].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    df[f'{value_col}_rolling_std_{window}'] = df.groupby(group_col)[value_col].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "    )\n",
    "    \n",
    "    # Calculate z-score based on rolling statistics\n",
    "    df[f'{value_col}_zscore_{window}'] = (\n",
    "        df[value_col] - df[f'{value_col}_rolling_mean_{window}']\n",
    "    ) / (df[f'{value_col}_rolling_std_{window}'] + 1e-8)  # Add small value to avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to key metrics\n",
    "asn_df = create_rolling_features(asn_df, 'asn_count')\n",
    "print(\"Added rolling features to ASN data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create change features\n",
    "def create_change_features(df, value_col, group_col='a_country_iso2'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Day-over-day change\n",
    "    df[f'{value_col}_pct_change'] = df.groupby(group_col)[value_col].pct_change()\n",
    "    df[f'{value_col}_diff'] = df.groupby(group_col)[value_col].diff()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to key metrics\n",
    "asn_df = create_change_features(asn_df, 'asn_count')\n",
    "print(\"Added change features to ASN data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore some patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Basic Statistics for ASN Count:\")\n",
    "print(asn_df['asn_count'].describe())\n",
    "\n",
    "# Countries with most records\n",
    "country_counts = asn_df['a_country_iso2'].value_counts()\n",
    "print(f\"\\nCountries with most records:\")\n",
    "print(country_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ASN count trends for top countries\n",
    "top_countries = asn_df['a_country_iso2'].value_counts().head(6).index\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, country in enumerate(top_countries):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    country_data = asn_df[asn_df['a_country_iso2'] == country]\n",
    "    plt.plot(country_data['a_date'], country_data['asn_count'], label=country)\n",
    "    plt.title(f'ASN Count Trends - {country}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('ASN Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data sources\n",
    "print(\"Merging data sources...\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "asn_df_renamed = asn_df.rename(columns={'a_date': 'date', 'a_country_iso2': 'country'})\n",
    "conn_df_renamed = conn_df.rename(columns={'asn_country': 'country'})\n",
    "stat_df_renamed = stat_df.rename(columns={'cs_country_iso2': 'country', 'cs_stats_timestamp': 'date'})\n",
    "\n",
    "# Merge ASN and connectivity data\n",
    "merged_df = pd.merge(asn_df_renamed, conn_df_renamed, on=['country', 'date'], how='outer')\n",
    "print(f\"Merged ASN and connectivity: {merged_df.shape}\")\n",
    "\n",
    "# Merge with country stats\n",
    "final_df = pd.merge(merged_df, stat_df_renamed, on=['country', 'date'], how='outer')\n",
    "print(f\"Final merged dataset: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create censorship indicators based on connectivity drops\n",
    "def create_censorship_indicators(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create binary indicator for significant drops in connectivity\n",
    "    # Using foreign_neighbours_share if available, otherwise using asn_count\n",
    "    \n",
    "    # If we have foreign_neighbours_share, use that as the primary metric\n",
    "    if 'foreign_neighbours_share' in df.columns:\n",
    "        # Calculate rolling median for each country\n",
    "        df['foreign_conn_rolling_median'] = df.groupby('country')['foreign_neighbours_share'].transform(\n",
    "            lambda x: x.rolling(window=30, min_periods=7).median()\n",
    "        )\n",
    "        \n",
    "        # Create indicator for significant drop below baseline\n",
    "        df['censorship_indicator'] = (\n",
    "            (df['foreign_neighbours_share'] < df['foreign_conn_rolling_median'] * 0.7) &\n",
    "            (df['foreign_neighbours_share'].notna())\n",
    "        ).astype(int)\n",
    "        \n",
    "    # If we have asn_count, use that as an alternative metric\n",
    "    if 'asn_count' in df.columns:\n",
    "        # Calculate rolling median for ASN count\n",
    "        df['asn_count_rolling_median'] = df.groupby('country')['asn_count'].transform(\n",
    "            lambda x: x.rolling(window=30, min_periods=7).median()\n",
    "        )\n",
    "        \n",
    "        # Create indicator for significant drop in ASN count\n",
    "        df['asn_censorship_indicator'] = (\n",
    "            (df['asn_count'] < df['asn_count_rolling_median'] * 0.7) &\n",
    "            (df['asn_count'].notna())\n",
    "        ).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "final_df = create_censorship_indicators(final_df)\n",
    "print(\"Created censorship indicators\")\n",
    "\n",
    "# Count how many potential censorship events were detected\n",
    "if 'censorship_indicator' in final_df.columns:\n",
    "    print(f\"Potential censorship events detected: {final_df['censorship_indicator'].sum()}\")\n",
    "if 'asn_censorship_indicator' in final_df.columns:\n",
    "    print(f\"Potential ASN-based censorship events detected: {final_df['asn_censorship_indicator'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Preparation for ML\n",
    "\n",
    "Prepare the final dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final features for ML\n",
    "print(\"Preparing final dataset for ML...\")\n",
    "\n",
    "# Select numeric columns for ML\n",
    "numeric_cols = final_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Number of numeric features: {len(numeric_cols)}\")\n",
    "\n",
    "# Handle missing values\n",
    "final_df_ml = final_df.copy()\n",
    "final_df_ml = final_df_ml.fillna(final_df_ml.median(numeric_only=True))\n",
    "\n",
    "# Ensure date and country columns are preserved\n",
    "if 'date' in final_df.columns:\n",
    "    final_df_ml['date'] = final_df['date']\n",
    "if 'country' in final_df.columns:\n",
    "    final_df_ml['country'] = final_df['country']\n",
    "    \n",
    "print(f\"Final dataset shape: {final_df_ml.shape}\")\n",
    "print(f\"Final dataset columns: {final_df_ml.shape[1]}\")\n",
    "\n",
    "# Look at potential target variables\n",
    "target_cols = ['censorship_indicator', 'asn_censorship_indicator']\n",
    "for col in target_cols:\n",
    "    if col in final_df_ml.columns:\n",
    "        print(f\"{col}: {final_df_ml[col].sum()} positive events ({final_df_ml[col].mean():.4f} ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data for ML\n",
    "output_path = '../data/processed/preprocessed_for_ml.csv'\n",
    "final_df_ml.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed data saved to {output_path}\")\n",
    "\n",
    "# Show sample of preprocessed data\n",
    "print(\"\\nSample of preprocessed data:\")\n",
    "print(final_df_ml.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}